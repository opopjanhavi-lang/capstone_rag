
# ğŸ“„ RAG System for Web & PDF (Research Papers)

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline that can ingest:
- ğŸŒ Web pages
- ğŸ“‘ PDF research papers (including arXiv and non-arXiv PDFs)

It allows you to **ask questions** over the ingested content using an LLM + vector database.

---

## ğŸš€ Features

- Web scraping support
- PDF URL ingestion (auto-download)
- Robust PDF text extraction with fallbacks
- Text cleaning & chunking
- Vector storage using **ChromaDB**
- LLM-powered question answering (RAG)
- Debug-friendly chunk inspection

---

## ğŸ§± Project Structure

```
.
â”œâ”€â”€ main.py
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ web_crawler.py
â”‚   â””â”€â”€ pdf_scraper.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ pdf_url_handler.py
â”‚   â”œâ”€â”€ text_chunker.py
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ vector_store.py
â”‚   â””â”€â”€ rag_chain.py
â”œâ”€â”€ chroma_db/
â””â”€â”€ README.md
```

---

## ğŸ” RAG Pipeline Flow (End-to-End)

Below is the **complete flow of the RAG pipeline** implemented in this project:

### 1ï¸âƒ£ Source Input
User provides a source:
- Web URL  
- PDF URL (e.g. research paper)
- Local PDF file

```text
User â†’ SOURCE (URL / PDF)
```

---

### 2ï¸âƒ£ Source Detection
The system detects the source type:

- `http + .pdf` â†’ PDF URL
- `http` â†’ Web page
- `.pdf` â†’ Local PDF

Handled inside:
```
main.py â†’ ingest_source()
```

---

### 3ï¸âƒ£ PDF URL Handling (if applicable)

For PDF URLs:
- PDF is downloaded locally
- Temporary file path is created

```
utils/pdf_url_handler.py
```

---

### 4ï¸âƒ£ Text Extraction

Depending on the source:

#### ğŸŒ Web
- HTML scraped
- Boilerplate removed

#### ğŸ“‘ PDF
- pdfplumber (text mode)
- pdfplumber (word-flow mode) â†’ fallback
- PyPDF2 â†’ final fallback

Handled in:
```
tools/pdf_scraper.py
```

---

### 5ï¸âƒ£ Cleaning & Section Filtering

The extracted text is:
- Lowercased
- References, tables, figures removed
- Equations stripped
- Abstract / Introduction / Conclusion preferred

This ensures **LLM-friendly text**.

---

### 6ï¸âƒ£ Chunking

Cleaned text is split using:
- RecursiveCharacterTextSplitter
- Overlapping chunks for context preservation

```
utils/text_chunker.py
```

Example:
```text
Chunk size: 500
Overlap: 100
```

---

### 7ï¸âƒ£ Vector Ingestion

Each chunk is:
- Embedded
- Stored in ChromaDB
- Tagged with source metadata

```
utils/ingest.py
```

---

### 8ï¸âƒ£ Query Time (RAG)

When a user asks a question:

1. Question â†’ embedding
2. Top-k relevant chunks retrieved
3. Context + question sent to LLM
4. Final grounded answer returned

```
utils/rag_chain.py
```

```text
User Question
   â†“
Vector Search
   â†“
Relevant Chunks
   â†“
LLM Prompt
   â†“
Answer
```

---

## ğŸ§ª Example Usage

```python
SOURCE = "https://arxiv.org/pdf/1706.03762.pdf"
ingest_source(SOURCE)
chat_loop()
```

Ask:
```
What problem does this paper aim to solve?
```

---

## âš ï¸ Note on arXiv PDFs

arXiv PDFs often:
- Lack proper spacing
- Use ligatures
- Embed text as glyphs

This project mitigates that using:
- `extract_words(use_text_flow=True)`
- Multiple fallback extractors

Non-arXiv PDFs usually parse perfectly.

---

## ğŸ§¹ Resetting the Database

To start fresh:
```bash
rm -rf chroma_db/
```

---

## ğŸ“Œ Future Improvements

- Layout-aware PDF parsing (GROBID)
- Table-aware chunking
- Section-level metadata
- Hybrid search (BM25 + embeddings)

---

## ğŸ‘¨â€ğŸ’» Author

Built as a **learning-first, production-style RAG system**.

---

â­ If this helped you, consider starring the repo!
